<h2>Emotion Voice Recognition: Detecting Human Emotions from Vocal Signals</h2>

This research aims to develop a system that detects human emotions from voice by analyzing vocal features such as tone, pitch, rhythm and Speech patterns. Using machine learning models trained in pre recorded  emotional speech datasets, the system will classify emotions like happiness, sadness, anger, fear and many more. A key application is in mental health monitoring, where voice-based emotion detection can offer non-invasive support and early intervention.

The project will result in a working model for real-time emotion recognition filtering any and all background noises, helping all ages identify the emotions that they are feeling and recognize the emotions exhibited in others to improve emotional literacy.

The interface will display the percentage of what emotion is being exhibited by each speaker, for example 55% neutral and 45% fear. The Interface can also alert the user when certain negative emotions, such as anxiety, start to rise to critical levels and remind the user to breathe or calm down before continuing with what they are doing.

Users will be able to input their vocal audios as well, to train their manipulation of emotional tone and speech. Audio samples of emotions can also be outputted for users to understand what certain emotions exhibited by others will sound like.

Objectives for this project is as follows:

1. To develop a machine learning framework capable of accurately detecting human emotions from vocal signals.
2. To promote emotional literacy and self-awareness among Malaysian youth and adults.
3. To support mental health monitoring through non-invasive, real-time emotion detection.
4. To provide a user-friendly interface that encourages proactive emotional regulation and reflection.


<h3>Testing software accuracy</h3>
<h4>Number and type of participants</h4>

The study will involve a minimum of 15 participants, consisting of university students and family members aged between 7 and 18 years old. Participants will be selected from diverse backgrounds, with no bias in terms of gender, race, appearance or socioeconomic status. Since the speech samples will be analyzed in English, all participants will be required to demonstrate a basic level of English-speaking proficiency. Parental or guardian consent will be obtained for participants under 18 years of age.

<h4>Number and duration of activities conducted</h4>

The research activities will be divided into two to three sessions per participant:

Spontaneous Speech Session - Participants will engage in a normal conversation with the interviewer while the program detects and displays emotional responses in real time.
Intentional Emotion Session - Participants will be asked to intentionally express specific emotions (e.g., happiness, sadness, anger) to test the system's recognition accuracy.
Emotion Recognition Session - Participants will listen to pre-recorded emotional speech samples and attempt to identify the emotions presented.

Each semi-structured interview or questionnaire session will last approximately 15 to 30 minutes, depending on the participant's age and comfort level.
